{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import spacy\n",
    "import tokenizations\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "from transformers import  DataCollatorForWholeWordMask\n",
    "from transformers.data.data_collator import tolist, _torch_collate_batch\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast, BertForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.integrations import WandbCallback, rewrite_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataProcessor():\n",
    "  def __init__(self, hf_dset, hf_tokenizer, max_length, text_col='text', lines_delimiter='\\n', minimize_data_size=True, apply_cleaning=True):\n",
    "    self.hf_tokenizer = hf_tokenizer\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    self._max_length = max_length\n",
    "    self._target_length = max_length\n",
    "\n",
    "    self.hf_dset = hf_dset\n",
    "    self.text_col = text_col\n",
    "    self.lines_delimiter = lines_delimiter\n",
    "    self.minimize_data_size = minimize_data_size\n",
    "    self.apply_cleaning = apply_cleaning\n",
    "    pos_classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "    self.pos_hash = {c: i for i, c in enumerate(pos_classes)}\n",
    "\n",
    "  def map(self, **kwargs) -> datasets.arrow_dataset.Dataset:\n",
    "    num_proc = kwargs.pop('num_proc', os.cpu_count())\n",
    "    cache_file_name = kwargs.pop('cache_file_name', None)\n",
    "    if cache_file_name is not None:\n",
    "        if not cache_file_name.endswith('.arrow'): \n",
    "            cache_file_name += '.arrow'        \n",
    "        if '/' not in cache_file_name: \n",
    "            cache_dir = os.path.abspath(os.path.dirname(self.hf_dset.cache_files[0]['filename']))\n",
    "            cache_file_name = os.path.join(cache_dir, cache_file_name)\n",
    "\n",
    "    return self.hf_dset.map(\n",
    "        function=self,\n",
    "        batched=True,\n",
    "        cache_file_name=cache_file_name,\n",
    "        remove_columns=self.hf_dset.column_names,\n",
    "        disable_nullable=True,\n",
    "        input_columns=[self.text_col],\n",
    "        writer_batch_size=10**4,\n",
    "        num_proc=num_proc,\n",
    "        **kwargs     \n",
    "    )\n",
    "\n",
    "  def __call__(self, texts):\n",
    "    if self.minimize_data_size: new_example = {'input_ids':[], 'sentA_length':[], 'pos_subword_info':[]}\n",
    "    else: new_example = {'input_ids':[], 'input_mask': [], 'segment_ids': []}\n",
    "\n",
    "    for text in texts: # for every doc\n",
    "      \n",
    "      for line in re.split(self.lines_delimiter, text): # for every paragraph\n",
    "        \n",
    "        if re.fullmatch(r'\\s*', line): continue # empty string or string with all space characters\n",
    "        if self.apply_cleaning and self.filter_out(line): continue\n",
    "        \n",
    "        example = self.add_line(line)\n",
    "        if example:\n",
    "          for k,v in example.items(): new_example[k].append(v)\n",
    "      \n",
    "      if self._current_length != 0:\n",
    "        example = self._create_example()\n",
    "        for k,v in example.items(): new_example[k].append(v)\n",
    "\n",
    "    return new_example\n",
    "\n",
    "  def filter_out(self, line):\n",
    "    if len(line) < 80: return True\n",
    "    return False \n",
    "\n",
    "  def clean(self, line):\n",
    "    # () is remainder after link in it filtered out\n",
    "    return line.strip().replace(\"\\n\", \" \").replace(\"()\",\"\")\n",
    "\n",
    "  def add_line(self, line):\n",
    "    \"\"\"Adds a line of text to the current example being built.\"\"\"\n",
    "    line = self.clean(line)\n",
    "    tokens = self.hf_tokenizer.tokenize(line, max_length=512, truncation=True)\n",
    "    tokids = self.hf_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    self._current_sentences.append(tokids)\n",
    "    self._current_length += len(tokids)\n",
    "    if self._current_length >= self._target_length:\n",
    "      return self._create_example()\n",
    "    return None\n",
    "\n",
    "  def _create_example(self):\n",
    "    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n",
    "    # small chance to only have one segment as in classification tasks\n",
    "    if random.random() < 0.1:\n",
    "      first_segment_target_length = 100000\n",
    "    else:\n",
    "      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n",
    "      first_segment_target_length = (self._target_length - 3) // 2\n",
    "\n",
    "    first_segment = []\n",
    "    second_segment = []\n",
    "    for sentence in self._current_sentences:\n",
    "      # the sentence goes to the first segment if (1) the first segment is\n",
    "      # empty, (2) the sentence doesn't put the first segment over length or\n",
    "      # (3) 50% of the time when it does put the first segment over length\n",
    "      if (len(first_segment) == 0 or\n",
    "          len(first_segment) + len(sentence) < first_segment_target_length or\n",
    "          (len(second_segment) == 0 and\n",
    "           len(first_segment) < first_segment_target_length and\n",
    "           random.random() < 0.5)):\n",
    "        first_segment += sentence\n",
    "      else:\n",
    "        second_segment += sentence\n",
    "\n",
    "    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n",
    "    first_segment = first_segment[:self._max_length - 2]\n",
    "    second_segment = second_segment[:max(0, self._max_length -\n",
    "                                         len(first_segment) - 3)]\n",
    "\n",
    "    # prepare to start building the next example\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    # small chance for random-length instead of max_length-length example\n",
    "    if random.random() < 0.05:\n",
    "      self._target_length = random.randint(5, self._max_length)\n",
    "    else:\n",
    "      self._target_length = self._max_length\n",
    "\n",
    "    return self._make_example(first_segment, second_segment)\n",
    "\n",
    "  def _make_example(self, first_segment, second_segment):\n",
    "    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n",
    "    input_ids = [self.hf_tokenizer.cls_token_id] + first_segment + [self.hf_tokenizer.sep_token_id]\n",
    "\n",
    "    bert_tokens = self.hf_tokenizer.convert_ids_to_tokens(first_segment)\n",
    "    sentence = self.hf_tokenizer.decode(first_segment)\n",
    "\n",
    "    with pos_tagger.select_pipes(enable=['morphologizer', 'tok2vec', 'tagger', 'attribute_ruler']):\n",
    "      spacy_doc = pos_tagger(sentence)\n",
    "    spacy_tokens = [t.text for t in spacy_doc]\n",
    "    pos = torch.tensor([self.pos_hash[t.pos_] for t in spacy_doc])\n",
    "\n",
    "    # align spacy_tokens to bert_tokens\n",
    "    a2b, b2a = tokenizations.get_alignments(spacy_tokens, bert_tokens)\n",
    "\n",
    "    count = 0\n",
    "    align_index = []\n",
    "    token_top = -1\n",
    "    for i in range(len(spacy_tokens)):\n",
    "      for j in a2b[i]:\n",
    "        if j > token_top:\n",
    "          align_index.append(count)\n",
    "      count += 1\n",
    "      token_top = a2b[i][-1]\n",
    "    \n",
    "    align_index = torch.tensor(align_index)\n",
    "    # assign pos to bert_tokens\n",
    "    pos_subword_info = torch.index_select(pos, dim=0, index=align_index)\n",
    "    pos_subword_info = [-1] + pos_subword_info.tolist() + [-1]\n",
    "\n",
    "    sentA_length = len(input_ids)\n",
    "    segment_ids = [0] * sentA_length\n",
    "    assert len(input_ids) == len(pos_subword_info)\n",
    "\n",
    "    # if second_segment:\n",
    "    #   input_ids += second_segment + [self.hf_tokenizer.sep_token_id]\n",
    "    #   segment_ids += [1] * (len(second_segment) + 1)\n",
    "\n",
    "    if self.minimize_data_size:\n",
    "      return {\n",
    "        'input_ids': input_ids,\n",
    "        'sentA_length': sentA_length,\n",
    "        'pos_subword_info': pos_subword_info\n",
    "      }\n",
    "    else:\n",
    "      input_mask = [1] * len(input_ids)\n",
    "      input_ids += [0] * (self._max_length - len(input_ids))\n",
    "      input_mask += [0] * (self._max_length - len(input_mask))\n",
    "      segment_ids += [0] * (self._max_length - len(segment_ids))\n",
    "      return {\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4824a4add94b9aad93bfb47246fc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceadce0b2b1047e49be26187cb61a00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0fd02fd4044fda950d0014fb7c3335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8ef7b72bfa4099ac47b8b145edca8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_tokenizer = BertTokenizerFast.from_pretrained(f\"bert-base-uncased\")\n",
    "BertProcessor = partial(BertDataProcessor, hf_tokenizer=hf_tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0387b77d7345c7b122bf9a47d45f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/wikitext-103-raw-v1 to /home/pasitt/work/ALBEF/parquet/wikitext-103-raw-v1-7bb180478b704b56/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951d1193d42a456993df5235585e9bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6072d933e1ab421ba88c846b47574ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b9802a4ee646e6b1b693e407950b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki = datasets.load_dataset('wikitext', 'default', cache_dir='./')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_wiki = BertProcessor(wiki).map(cache_file_name=f\"bert_wikitext_128.arrow\", num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_wiki[0]['pos_subword_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertProcessor(wiki).pos_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
