{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import spacy\n",
    "import tokenizations\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "from transformers import  DataCollatorForWholeWordMask\n",
    "from transformers.data.data_collator import tolist, _torch_collate_batch\n",
    "\n",
    "from transformers import BertConfig, BertTokenizerFast, BertForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.integrations import WandbCallback, rewrite_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataProcessor():\n",
    "  def __init__(self, hf_dset, hf_tokenizer, max_length, text_col='text', lines_delimiter='\\n', minimize_data_size=True, apply_cleaning=True):\n",
    "    self.hf_tokenizer = hf_tokenizer\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    self._max_length = max_length\n",
    "    self._target_length = max_length\n",
    "\n",
    "    self.hf_dset = hf_dset\n",
    "    self.text_col = text_col\n",
    "    self.lines_delimiter = lines_delimiter\n",
    "    self.minimize_data_size = minimize_data_size\n",
    "    self.apply_cleaning = apply_cleaning\n",
    "    pos_classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "    self.pos_hash = {c: i for i, c in enumerate(pos_classes)}\n",
    "\n",
    "  def map(self, **kwargs) -> datasets.arrow_dataset.Dataset:\n",
    "    num_proc = kwargs.pop('num_proc', os.cpu_count())\n",
    "    cache_file_name = kwargs.pop('cache_file_name', None)\n",
    "    if cache_file_name is not None:\n",
    "        if not cache_file_name.endswith('.arrow'): \n",
    "            cache_file_name += '.arrow'        \n",
    "        if '/' not in cache_file_name: \n",
    "            cache_dir = os.path.abspath(os.path.dirname(self.hf_dset.cache_files[0]['filename']))\n",
    "            cache_file_name = os.path.join(cache_dir, cache_file_name)\n",
    "\n",
    "    return self.hf_dset.map(\n",
    "        function=self,\n",
    "        batched=True,\n",
    "        cache_file_name=cache_file_name,\n",
    "        remove_columns=self.hf_dset.column_names,\n",
    "        disable_nullable=True,\n",
    "        input_columns=[self.text_col],\n",
    "        writer_batch_size=10**4,\n",
    "        num_proc=num_proc,\n",
    "        **kwargs     \n",
    "    )\n",
    "\n",
    "  def __call__(self, texts):\n",
    "    if self.minimize_data_size: new_example = {'input_ids':[], 'sentA_length':[], 'pos_subword_info':[]}\n",
    "    else: new_example = {'input_ids':[], 'input_mask': [], 'segment_ids': []}\n",
    "\n",
    "    for text in texts: # for every doc\n",
    "      \n",
    "      for line in re.split(self.lines_delimiter, text): # for every paragraph\n",
    "        \n",
    "        if re.fullmatch(r'\\s*', line): continue # empty string or string with all space characters\n",
    "        if self.apply_cleaning and self.filter_out(line): continue\n",
    "        \n",
    "        example = self.add_line(line)\n",
    "        if example:\n",
    "          for k,v in example.items(): new_example[k].append(v)\n",
    "      \n",
    "      if self._current_length != 0:\n",
    "        example = self._create_example()\n",
    "        for k,v in example.items(): new_example[k].append(v)\n",
    "\n",
    "    return new_example\n",
    "\n",
    "  def filter_out(self, line):\n",
    "    if len(line) < 80: return True\n",
    "    return False \n",
    "\n",
    "  def clean(self, line):\n",
    "    # () is remainder after link in it filtered out\n",
    "    return line.strip().replace(\"\\n\", \" \").replace(\"()\",\"\")\n",
    "\n",
    "  def add_line(self, line):\n",
    "    \"\"\"Adds a line of text to the current example being built.\"\"\"\n",
    "    line = self.clean(line)\n",
    "    tokens = self.hf_tokenizer.tokenize(line, max_length=512, truncation=True)\n",
    "    tokids = self.hf_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    self._current_sentences.append(tokids)\n",
    "    self._current_length += len(tokids)\n",
    "    if self._current_length >= self._target_length:\n",
    "      return self._create_example()\n",
    "    return None\n",
    "\n",
    "  def _create_example(self):\n",
    "    \"\"\"Creates a pre-training example from the current list of sentences.\"\"\"\n",
    "    # small chance to only have one segment as in classification tasks\n",
    "    if random.random() < 0.1:\n",
    "      first_segment_target_length = 100000\n",
    "    else:\n",
    "      # -3 due to not yet having [CLS]/[SEP] tokens in the input text\n",
    "      first_segment_target_length = (self._target_length - 3) // 2\n",
    "\n",
    "    first_segment = []\n",
    "    second_segment = []\n",
    "    for sentence in self._current_sentences:\n",
    "      # the sentence goes to the first segment if (1) the first segment is\n",
    "      # empty, (2) the sentence doesn't put the first segment over length or\n",
    "      # (3) 50% of the time when it does put the first segment over length\n",
    "      if (len(first_segment) == 0 or\n",
    "          len(first_segment) + len(sentence) < first_segment_target_length or\n",
    "          (len(second_segment) == 0 and\n",
    "           len(first_segment) < first_segment_target_length and\n",
    "           random.random() < 0.5)):\n",
    "        first_segment += sentence\n",
    "      else:\n",
    "        second_segment += sentence\n",
    "\n",
    "    # trim to max_length while accounting for not-yet-added [CLS]/[SEP] tokens\n",
    "    first_segment = first_segment[:self._max_length - 2]\n",
    "    second_segment = second_segment[:max(0, self._max_length -\n",
    "                                         len(first_segment) - 3)]\n",
    "\n",
    "    # prepare to start building the next example\n",
    "    self._current_sentences = []\n",
    "    self._current_length = 0\n",
    "    # small chance for random-length instead of max_length-length example\n",
    "    if random.random() < 0.05:\n",
    "      self._target_length = random.randint(5, self._max_length)\n",
    "    else:\n",
    "      self._target_length = self._max_length\n",
    "\n",
    "    return self._make_example(first_segment, second_segment)\n",
    "\n",
    "  def _make_example(self, first_segment, second_segment):\n",
    "    \"\"\"Converts two \"segments\" of text into a tf.train.Example.\"\"\"\n",
    "    input_ids = [self.hf_tokenizer.cls_token_id] + first_segment + [self.hf_tokenizer.sep_token_id]\n",
    "\n",
    "    bert_tokens = self.hf_tokenizer.convert_ids_to_tokens(first_segment)\n",
    "    sentence = self.hf_tokenizer.decode(first_segment)\n",
    "\n",
    "    with pos_tagger.select_pipes(enable=['morphologizer', 'tok2vec', 'tagger', 'attribute_ruler']):\n",
    "      spacy_doc = pos_tagger(sentence)\n",
    "    spacy_tokens = [t.text for t in spacy_doc]\n",
    "    pos = torch.tensor([self.pos_hash[t.pos_] for t in spacy_doc])\n",
    "\n",
    "    # align spacy_tokens to bert_tokens\n",
    "    a2b, b2a = tokenizations.get_alignments(spacy_tokens, bert_tokens)\n",
    "\n",
    "    count = 0\n",
    "    align_index = []\n",
    "    token_top = -1\n",
    "    for i in range(len(spacy_tokens)):\n",
    "      for j in a2b[i]:\n",
    "        if j > token_top:\n",
    "          align_index.append(count)\n",
    "      count += 1\n",
    "      token_top = a2b[i][-1]\n",
    "    \n",
    "    align_index = torch.tensor(align_index)\n",
    "    # assign pos to bert_tokens\n",
    "    pos_subword_info = torch.index_select(pos, dim=0, index=align_index)\n",
    "    pos_subword_info = [-1] + pos_subword_info.tolist() + [-1]\n",
    "\n",
    "    sentA_length = len(input_ids)\n",
    "    segment_ids = [0] * sentA_length\n",
    "    assert len(input_ids) == len(pos_subword_info)\n",
    "\n",
    "    # if second_segment:\n",
    "    #   input_ids += second_segment + [self.hf_tokenizer.sep_token_id]\n",
    "    #   segment_ids += [1] * (len(second_segment) + 1)\n",
    "\n",
    "    if self.minimize_data_size:\n",
    "      return {\n",
    "        'input_ids': input_ids,\n",
    "        'sentA_length': sentA_length,\n",
    "        'pos_subword_info': pos_subword_info\n",
    "      }\n",
    "    else:\n",
    "      input_mask = [1] * len(input_ids)\n",
    "      input_ids += [0] * (self._max_length - len(input_ids))\n",
    "      input_mask += [0] * (self._max_length - len(input_mask))\n",
    "      segment_ids += [0] * (self._max_length - len(segment_ids))\n",
    "      return {\n",
    "        'input_ids': input_ids,\n",
    "        'input_mask': input_mask,\n",
    "        'segment_ids': segment_ids,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = BertTokenizerFast.from_pretrained(f\"bert-base-uncased\")\n",
    "BertProcessor = partial(BertDataProcessor, hf_tokenizer=hf_tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2027f45146244e0894fe98a74e88f2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19650dccd374b21a8ccb791377a0fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f02b2ef09ef40c99e942e687fdeaeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki = datasets.load_dataset('wikitext', 'default', cache_dir='./')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series \\' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by <unk> The original scenario was written Kazuki Yamanobe , while the script was written by Hiroyuki Fujii , Koichi Majima , <unk> Miyagi , Seiki <unk> and Takayuki <unk> . Its story was darker and more somber than that of its predecessor . \\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb14b66d1004ebabc9b542a74bc0c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e_wiki = BertProcessor(wiki).map(cache_file_name=f\"bert_wikitext_128.arrow\", num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'sentA_length', 'pos_subword_info'],\n",
       "    num_rows: 762789\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " 15,\n",
       " 15,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 16,\n",
       " 6,\n",
       " 6,\n",
       " 16,\n",
       " 7,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 4,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 1,\n",
       " 11,\n",
       " 12,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 1,\n",
       " 15,\n",
       " 7,\n",
       " 7,\n",
       " 15,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 12,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 12,\n",
       " 15,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 15,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 10,\n",
       " 7,\n",
       " 12,\n",
       " 5,\n",
       " 7,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 15,\n",
       " 5,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 12,\n",
       " 12,\n",
       " 5,\n",
       " -1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_wiki[0]['pos_subword_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertProcessor(wiki).pos_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
