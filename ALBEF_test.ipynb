{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87613ba6-aefe-4b9b-896c-12a0dabc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "from typing import Any, Tuple, Callable, Optional, List\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from torchvision import transforms\n",
    "from ruamel.yaml import YAML\n",
    "import os\n",
    "import utils\n",
    "import spacy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "cuda_card = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b10a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611932f8-dbcc-4fa2-ac95-2a1c847234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCustom(CocoCaptions): \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, annFile, transform, target_transform, transforms)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.annotations = json.load(open(annFile))\n",
    "        self.num_captions = len(self.annotations['annotations'])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, id, target\n",
    "\n",
    "    def getAnnotationRange(self, index: int, count: int) -> List[Any]:\n",
    "        return [self.annotations['annotations'][index]['caption'] for index in range(index, index+count)]\n",
    "\n",
    "    def getImgIdFromAnnotationIndex(self, annotation_index: int) -> int:\n",
    "        return self.annotations['annotations'][annotation_index]['image_id']\n",
    "    \n",
    "    def buildFaissIndex(self, text_encoder, tokenize, batch_size, nlist) :\n",
    "        tokenized = tokenize(self.getAnnotationRange(0, batch_size)).cuda(cuda_card)\n",
    "        encoded_captions = normalize_vector(text_encoder(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "        vector_dimension = encoded_captions.shape[1]\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist)\n",
    "        index.train(encoded_captions)\n",
    "        index.add(encoded_captions)\n",
    "        \n",
    "        for i in tqdm(range(batch_size, self.num_captions - batch_size, batch_size)):\n",
    "            tokenized = clip.tokenize(self.getAnnotationRange(i, batch_size)).cuda(cuda_card)\n",
    "            encoded_captions = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "            index.add(encoded_captions)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = YAML(typ='rt')\n",
    "config = yaml.load(open(\"./configs/Pretrain.yaml\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc1a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  5000\n",
      "Image Size: torch.Size([3, 256, 256])\n",
      "Captions: [\"A stop sign is mounted upside-down on it's post. \", 'A stop sign that is hanging upside down.', 'An upside down stop sign by the road.', 'a stop sign put upside down on a metal pole ', 'A stop sign installed upside down on a street corner']\n",
      "Image Id: 724\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def collate_coco(data):\n",
    "    images, image_ids, captions = zip(*data)\n",
    "\n",
    "    # Stack image tensors into a single batch tensor\n",
    "    images = torch.stack(images, dim=0)  # Shape: (batch_size, C, H, W)\n",
    "\n",
    "    # Convert image_ids into a list\n",
    "    image_ids = list(image_ids)  # Or use torch.tensor(image_ids) if they are numeric\n",
    "\n",
    "    # Collate captions\n",
    "    # If captions have variable lengths, you can return them as a list\n",
    "    first_captions = [cap[0] for cap in captions]  # First caption of each sample\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"image_ids\": image_ids,\n",
    "        \"captions\": first_captions,  # Only the first caption\n",
    "    }\n",
    "\n",
    "path = '../../Dataset/CV/mscoco/2017'\n",
    "cocoCaptions = CocoCustom(root = path + '/val2017',\n",
    "                        annFile = path + '/annotations/captions_val2017.json',\n",
    "                        transform=test_transform)\n",
    "\n",
    "print('Number of samples: ', len(cocoCaptions))\n",
    "img, img_id, target = cocoCaptions[3]\n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(cocoCaptions, collate_fn=collate_coco, batch_size=batch_size)\n",
    "\n",
    "print(\"Image Size:\", img.size())\n",
    "print(\"Captions:\", target)\n",
    "print(\"Image Id:\", img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b98662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1037, 2450, 4832, 1999, 1996, 7759, 2181, 2012, 1996, 2795, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'pos_ids': [-1, 5, 7, 15, 1, 5, 7, 7, 1, 5, 7, 12, -1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_pretrain import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "import tokenizations\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "index = 0\n",
    "caption = cocoCaptions[index][2][0]\n",
    "out = tokenizer(caption)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d7f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /home/pasitt/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
      "100%|██████████| 330M/330M [00:29<00:00, 11.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 256\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['head.weight', 'head.bias'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0db83604d24c6c81a06fc989c4e392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = ALBEF(config=config, text_encoder=\"bert-base-uncased\", tokenizer=tokenizer)\n",
    "\n",
    "model = model.cuda(cuda_card)\n",
    "model.eval()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86de371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = {\n",
    "    'distributed': False\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(data_loader))\n",
    "    images = batch['images']\n",
    "    captions = tokenizer(batch['captions'], padding='longest', truncation=True, max_length=25, return_tensors=\"pt\")\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    output = model(images, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42171f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  5,  7, 15,  1,  5,  7,  7,  1,  5,  7, 12, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['pos_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437c21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A woman stands in the dining area at the table.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['captions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66be7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "pos_hash = {c: i for i, c in enumerate(pos_classes)}\n",
    "\n",
    "with pos_tagger.select_pipes(enable=['morphologizer', 'tok2vec', 'tagger', 'attribute_ruler']):\n",
    "    spacy_doc = pos_tagger(batch['captions'][4])\n",
    "spacy_pos = torch.tensor([pos_hash[t.pos_] for t in spacy_doc])\n",
    "spacy_tokens = [t.text for t in spacy_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0fc8b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " ',',\n",
       " 'each',\n",
       " 'a',\n",
       " 'different',\n",
       " 'color',\n",
       " ',',\n",
       " 'snuggling',\n",
       " 'together',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea644b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " ',',\n",
       " 'each',\n",
       " 'a',\n",
       " 'different',\n",
       " 'color',\n",
       " ',',\n",
       " 's',\n",
       " '##nu',\n",
       " '##gg',\n",
       " '##ling',\n",
       " 'together',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens = tokenizer.convert_ids_to_tokens(captions.input_ids[4])\n",
    "bert_tokens[1:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a7f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9, 10, 11, 12], [13], [14]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2b, b2a = tokenizations.get_alignments(spacy_tokens, bert_tokens[1:16])\n",
    "a2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e2f257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  7.,  7., 12.,  5.,  5.,  0.,  7., 12., 15., 15., 15., 15.,  2.,\n",
       "        12.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ids = torch.zeros(len(bert_tokens[1:16]))\n",
    "for idx, id_map in enumerate(a2b):\n",
    "    for i in id_map:\n",
    "        pos_ids[i] = spacy_pos[idx]\n",
    "pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50f20f7-e02c-4795-8004-32077798aed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 131)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/cc3m_validation.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 131)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "temp = json.load(open('data/cc3m_validation.json','r'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
