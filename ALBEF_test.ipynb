{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87613ba6-aefe-4b9b-896c-12a0dabc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "from typing import Any, Tuple, Callable, Optional, List\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from torchvision import transforms\n",
    "from ruamel.yaml import YAML\n",
    "import os\n",
    "import utils\n",
    "import spacy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "cuda_card = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b10a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611932f8-dbcc-4fa2-ac95-2a1c847234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCustom(CocoCaptions): \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, annFile, transform, target_transform, transforms)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.annotations = json.load(open(annFile))\n",
    "        self.num_captions = len(self.annotations['annotations'])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, id, target\n",
    "\n",
    "    def getAnnotationRange(self, index: int, count: int) -> List[Any]:\n",
    "        return [self.annotations['annotations'][index]['caption'] for index in range(index, index+count)]\n",
    "\n",
    "    def getImgIdFromAnnotationIndex(self, annotation_index: int) -> int:\n",
    "        return self.annotations['annotations'][annotation_index]['image_id']\n",
    "    \n",
    "    def buildFaissIndex(self, text_encoder, tokenize, batch_size, nlist) :\n",
    "        tokenized = tokenize(self.getAnnotationRange(0, batch_size)).cuda(cuda_card)\n",
    "        encoded_captions = normalize_vector(text_encoder(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "        vector_dimension = encoded_captions.shape[1]\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist)\n",
    "        index.train(encoded_captions)\n",
    "        index.add(encoded_captions)\n",
    "        \n",
    "        for i in tqdm(range(batch_size, self.num_captions - batch_size, batch_size)):\n",
    "            tokenized = clip.tokenize(self.getAnnotationRange(i, batch_size)).cuda(cuda_card)\n",
    "            encoded_captions = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "            index.add(encoded_captions)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = YAML(typ='rt')\n",
    "config = yaml.load(open(\"./configs/Pretrain.yaml\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc1a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  5000\n",
      "Image Size: torch.Size([3, 256, 256])\n",
      "Captions: [\"A stop sign is mounted upside-down on it's post. \", 'A stop sign that is hanging upside down.', 'An upside down stop sign by the road.', 'a stop sign put upside down on a metal pole ', 'A stop sign installed upside down on a street corner']\n",
      "Image Id: 724\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def collate_coco(data):\n",
    "    images, image_ids, captions = zip(*data)\n",
    "\n",
    "    # Stack image tensors into a single batch tensor\n",
    "    images = torch.stack(images, dim=0)  # Shape: (batch_size, C, H, W)\n",
    "\n",
    "    # Convert image_ids into a list\n",
    "    image_ids = list(image_ids)  # Or use torch.tensor(image_ids) if they are numeric\n",
    "\n",
    "    # Collate captions\n",
    "    # If captions have variable lengths, you can return them as a list\n",
    "    first_captions = [cap[0] for cap in captions]  # First caption of each sample\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"image_ids\": image_ids,\n",
    "        \"captions\": first_captions,  # Only the first caption\n",
    "    }\n",
    "\n",
    "path = '../../Dataset/CV/mscoco/2017'\n",
    "cocoCaptions = CocoCustom(root = path + '/val2017',\n",
    "                        annFile = path + '/annotations/captions_val2017.json',\n",
    "                        transform=test_transform)\n",
    "\n",
    "print('Number of samples: ', len(cocoCaptions))\n",
    "img, img_id, target = cocoCaptions[3]\n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(cocoCaptions, collate_fn=collate_coco, batch_size=batch_size)\n",
    "\n",
    "print(\"Image Size:\", img.size())\n",
    "print(\"Captions:\", target)\n",
    "print(\"Image Id:\", img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b98662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1037, 2450, 4832, 1999, 1996, 7759, 2181, 2012, 1996, 2795, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'pos_ids': [-1, 5, 7, 15, 1, 5, 7, 7, 1, 5, 7, 12, -1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_pretrain import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "import tokenizations\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "index = 0\n",
    "caption = cocoCaptions[index][2][0]\n",
    "out = tokenizer(caption)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2d7f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /home/pasitt/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
      "100%|██████████| 330M/330M [00:29<00:00, 11.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 196 to 256\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['head.weight', 'head.bias'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0db83604d24c6c81a06fc989c4e392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = ALBEF(config=config, text_encoder=\"bert-base-uncased\", tokenizer=tokenizer)\n",
    "\n",
    "model = model.cuda(cuda_card)\n",
    "model.eval()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86de371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = {\n",
    "    'distributed': False\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(data_loader))\n",
    "    images = batch['images']\n",
    "    captions = tokenizer(batch['captions'], padding='longest', truncation=True, max_length=25, return_tensors=\"pt\")\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    output = model(images, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42171f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  5,  7, 15,  1,  5,  7,  7,  1,  5,  7, 12, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions['pos_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437c21a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A woman stands in the dining area at the table.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['captions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66be7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "pos_hash = {c: i for i, c in enumerate(pos_classes)}\n",
    "\n",
    "with pos_tagger.select_pipes(enable=['morphologizer', 'tok2vec', 'tagger', 'attribute_ruler']):\n",
    "    spacy_doc = pos_tagger(batch['captions'][4])\n",
    "spacy_pos = torch.tensor([pos_hash[t.pos_] for t in spacy_doc])\n",
    "spacy_tokens = [t.text for t in spacy_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0fc8b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " ',',\n",
       " 'each',\n",
       " 'a',\n",
       " 'different',\n",
       " 'color',\n",
       " ',',\n",
       " 'snuggling',\n",
       " 'together',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea644b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three',\n",
       " 'teddy',\n",
       " 'bears',\n",
       " ',',\n",
       " 'each',\n",
       " 'a',\n",
       " 'different',\n",
       " 'color',\n",
       " ',',\n",
       " 's',\n",
       " '##nu',\n",
       " '##gg',\n",
       " '##ling',\n",
       " 'together',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens = tokenizer.convert_ids_to_tokens(captions.input_ids[4])\n",
    "bert_tokens[1:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a7f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9, 10, 11, 12], [13], [14]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2b, b2a = tokenizations.get_alignments(spacy_tokens, bert_tokens[1:16])\n",
    "a2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94e2f257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  7.,  7., 12.,  5.,  5.,  0.,  7., 12., 15., 15., 15., 15.,  2.,\n",
       "        12.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ids = torch.zeros(len(bert_tokens[1:16]))\n",
    "for idx, id_map in enumerate(a2b):\n",
    "    for i in id_map:\n",
    "        pos_ids[i] = spacy_pos[idx]\n",
    "pos_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
