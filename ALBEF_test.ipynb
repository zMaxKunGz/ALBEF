{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87613ba6-aefe-4b9b-896c-12a0dabc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from typing import Any, Tuple, Callable, Optional, List\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from torchvision import transforms\n",
    "from ruamel.yaml import YAML\n",
    "import os\n",
    "import utils\n",
    "import spacy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "cuda_card = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b10a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611932f8-dbcc-4fa2-ac95-2a1c847234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCustom(CocoCaptions): \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, annFile, transform, target_transform, transforms)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.annotations = json.load(open(annFile))\n",
    "        self.num_captions = len(self.annotations['annotations'])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, id, target\n",
    "\n",
    "    def getAnnotationRange(self, index: int, count: int) -> List[Any]:\n",
    "        return [self.annotations['annotations'][index]['caption'] for index in range(index, index+count)]\n",
    "\n",
    "    def getImgIdFromAnnotationIndex(self, annotation_index: int) -> int:\n",
    "        return self.annotations['annotations'][annotation_index]['image_id']\n",
    "    \n",
    "    def buildFaissIndex(self, text_encoder, tokenize, batch_size, nlist) :\n",
    "        tokenized = tokenize(self.getAnnotationRange(0, batch_size)).cuda(cuda_card)\n",
    "        encoded_captions = normalize_vector(text_encoder(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "        vector_dimension = encoded_captions.shape[1]\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist)\n",
    "        index.train(encoded_captions)\n",
    "        index.add(encoded_captions)\n",
    "        \n",
    "        for i in tqdm(range(batch_size, self.num_captions - batch_size, batch_size)):\n",
    "            tokenized = clip.tokenize(self.getAnnotationRange(i, batch_size)).cuda(cuda_card)\n",
    "            encoded_captions = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "            index.add(encoded_captions)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986b0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = YAML(typ='rt')\n",
    "config = yaml.load(open(\"./configs/Pretrain.yaml\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc1a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  40504\n",
      "Image Size: torch.Size([3, 256, 256])\n",
      "Captions: ['A loft bed with a dresser underneath it.', 'A bed and desk in a small room.', 'Wooden bed on top of a white dresser.', 'A bed sits on top of a dresser and a desk.', 'Bunk bed with a narrow shelf sitting underneath it. ']\n",
      "Image Id: 133\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def collate_coco(data):\n",
    "    images, image_ids, captions = zip(*data)\n",
    "\n",
    "    # Stack image tensors into a single batch tensor\n",
    "    images = torch.stack(images, dim=0)  # Shape: (batch_size, C, H, W)\n",
    "\n",
    "    # Convert image_ids into a list\n",
    "    image_ids = list(image_ids)  # Or use torch.tensor(image_ids) if they are numeric\n",
    "\n",
    "    # Collate captions\n",
    "    # If captions have variable lengths, you can return them as a list\n",
    "    first_captions = [cap[0] for cap in captions]  # First caption of each sample\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"image_ids\": image_ids,\n",
    "        \"captions\": first_captions,  # Only the first caption\n",
    "    }\n",
    "\n",
    "path = '../../Dataset/CV/mscoco/2014'\n",
    "cocoCaptions = CocoCustom(root = path + '/val2014',\n",
    "                        annFile = path + '/annotations/captions_val2014.json',\n",
    "                        transform=test_transform)\n",
    "\n",
    "print('Number of samples: ', len(cocoCaptions))\n",
    "img, img_id, target = cocoCaptions[3]\n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(cocoCaptions, collate_fn=collate_coco, batch_size=batch_size)\n",
    "\n",
    "print(\"Image Size:\", img.size())\n",
    "print(\"Captions:\", target)\n",
    "print(\"Image Id:\", img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b98662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasitt/work/pytorch/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/pasitt/work/pytorch/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "2025-06-03 15:57:02.632791: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-03 15:57:02.639294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748966222.647129 1237903 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748966222.649496 1237903 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-03 15:57:02.658755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cocoCaptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43mcocoCaptions\u001b[49m[index][\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m out \u001b[38;5;241m=\u001b[39m tokenizer(caption)\n\u001b[1;32m     11\u001b[0m out\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cocoCaptions' is not defined"
     ]
    }
   ],
   "source": [
    "from models.model_pretrain import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "import tokenizations\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "index = 0\n",
    "caption = cocoCaptions[index][2][0]\n",
    "out = tokenizer(caption)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = ALBEF(config=config, text_encoder=\"bert-base-uncased\", tokenizer=tokenizer)\n",
    "\n",
    "model = model.cuda(cuda_card)\n",
    "model.eval()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86de371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = {\n",
    "    'distributed': False\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "utils.init_distributed_mode(args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(data_loader))\n",
    "    images = batch['images']\n",
    "    captions = tokenizer(batch['captions'], padding='longest', truncation=True, max_length=25, return_tensors=\"pt\")\n",
    "    images = images.to(device)\n",
    "    captions = captions.to(device)\n",
    "    output = model(images, captions, masking_pos='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42171f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = captions.input_ids.clone()\n",
    "pos_ids = captions.pos_ids.clone()\n",
    "labels = input_ids.clone()\n",
    "# input_ids, labels = model.mask(input_ids, pos_ids, 30522, images.device, targets=labels, masking_pos=\"ADJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63ecd751-63a4-4b0e-9381-23de7b4ff4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking_pos_id = pos_hash[\"ADJ\"]\n",
    "masked_indices = torch.zeros(input_ids.shape)\n",
    "masked_indices[pos_ids==masking_pos_id] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cba86404-b2dc-454a-9571-c20f29c55eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  5,  0,  7,  1,  0,  5,  7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ids[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a2ad234-f7cd-48a2-9aa6-178bf2ffc1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_ids==masking_pos_id)[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "223ecc9b-c45e-40f7-8d37-41b2c547de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2048, 21025, 27528,  7959,  2015,  1999,  1037,  2282,  2007,\n",
       "         2111,  2559,  2012,  2068,  1012,   102,     0,     0,     0,     0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66be7256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'wire',\n",
       " 'metal',\n",
       " 'rack',\n",
       " 'holds',\n",
       " 'several',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'shoes',\n",
       " 'and',\n",
       " 'sandals']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_classes = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "pos_hash = {c: i for i, c in enumerate(pos_classes)}\n",
    "\n",
    "with pos_tagger.select_pipes(enable=['morphologizer', 'tok2vec', 'tagger', 'attribute_ruler']):\n",
    "    spacy_doc = pos_tagger(batch['captions'][0])\n",
    "spacy_pos = torch.tensor([pos_hash[t.pos_] for t in spacy_doc])\n",
    "spacy_tokens = [t.text for t in spacy_doc]\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7136303-e09c-4c38-95e1-72967f01cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ARO.dataset_zoo import VG_Relation, VG_Attribution, COCO_Order, Flickr30k_Order\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abad837b-32ab-49bc-9d3b-16e562dba04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256,256),interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "root_dir=\"/home/pasitt/Dataset/CV/ARO\"\n",
    "# Setting download=True will download the dataset to `root_dir` if it's not already there. \n",
    "# For VG-R and VG-A, this is a 1GB zip file that is a subset of GQA.\n",
    "\n",
    "vgr_dataset = VG_Relation(image_preprocess=preprocess, download=True, root_dir=root_dir)\n",
    "vga_dataset = VG_Attribution(image_preprocess=preprocess, download=True, root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625065c3-a2c6-4445-8a85-e814ea13bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/pasitt/Dataset/CV/mscoco/2014/coco_karpathy_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:03, 15.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coco_order_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO_Order\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_preprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/pasitt/Dataset/CV/mscoco/2014\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/work/ALBEF/ARO/dataset_zoo/aro_datasets.py:239\u001b[0m, in \u001b[0;36mCOCO_Order.__init__\u001b[0;34m(self, image_preprocess, root_dir, max_words, split, image_perturb_fn, download)\u001b[0m\n\u001b[1;32m    236\u001b[0m test_case[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [pre_caption(caption,max_words)]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perturb_fn \u001b[38;5;129;01min\u001b[39;00m perturb_functions:\n\u001b[0;32m--> 239\u001b[0m     test_case[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_options\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(pre_caption(\u001b[43mperturb_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m)\u001b[49m, max_words))\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_cases\u001b[38;5;241m.\u001b[39mappend(test_case)\n",
      "File \u001b[0;32m~/work/ALBEF/ARO/dataset_zoo/perturbations.py:70\u001b[0m, in \u001b[0;36mTextShuffler.shuffle_allbut_nouns_and_adj\u001b[0;34m(self, ex)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshuffle_allbut_nouns_and_adj\u001b[39m(\u001b[38;5;28mself\u001b[39m, ex):\n\u001b[0;32m---> 70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[1;32m     72\u001b[0m     text \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tokens)\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/ml/tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/spacy/ml/parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/layers/list2ragged.py:24\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackprop\u001b[39m(dYr: OutT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InT:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(InT, model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39munflatten(dYr\u001b[38;5;241m.\u001b[39mdata, dYr\u001b[38;5;241m.\u001b[39mlengths))\n\u001b[0;32m---> 24\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray1i\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs), lengths), backprop\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/backends/ops.py:710\u001b[0m, in \u001b[0;36mOps.asarray1i\u001b[0;34m(self, data, dtype)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21masarray1i\u001b[39m(\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Union[Ints1d, Sequence[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;241m*\u001b[39m, dtype: Optional[DTypes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ints1d:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Ints1d, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/work/pytorch/lib/python3.10/site-packages/thinc/backends/numpy_ops.pyx:74\u001b[0m, in \u001b[0;36mthinc.backends.numpy_ops.NumpyOps.asarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coco_order_dataset = COCO_Order(image_preprocess=preprocess, download=False, root_dir='/home/pasitt/Dataset/CV/mscoco/2014') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf16dd5-03dc-4145-bebc-6fc525478752",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_order_dataset = Flickr30k_Order(image_preprocess=preprocess, root_dir='/home/pasitt/Dataset/CV/flickr/', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e8754-4fdb-4ade-b912-70fe8cadb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vga_dataset[0]['caption_options']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
